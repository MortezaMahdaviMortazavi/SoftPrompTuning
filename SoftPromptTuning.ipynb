{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dsKy4jpCQFI",
        "outputId": "1d8ead75-292f-4627-e846-96004967b9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas torch transformers nltk numpy matplotlib\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E-xZtfiCkjj",
        "outputId": "e75ce011-9982-4713-85db-f37992d79575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Setting the seed in Hugging Face Transformers\n",
        "    import transformers\n",
        "    transformers.set_seed(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "G-0PUAIvlCo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Optimizations:\n",
        "\"\"\"\n",
        "  1 - Use flash attention and rewrite forward of it\n",
        "  2 - Use torch.compile\n",
        "  3 - use kv-cache\n",
        "  4 - use MixedPrecision training\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9SfxKR3-QV-N",
        "outputId": "1b12bf22-57e2-46bf-c939-56561f590b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  1 - Use flash attention and rewrite forward of it\\n  2 - Use torch.compile\\n  3 - use kv-cache\\n  4 - use MixedPrecision training\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "def load_model():\n",
        "    model_name = \"t5-small\"\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    # | Model Class                    | Encoder-Decoder | Vocabulary Logits for Generation | Typical Use Case                                 |\n",
        "    # |------------------------------- |-----------------|----------------------------------|------------------------------------------------- |\n",
        "    # | `T5ForConditionalGeneration`   | Yes             | Yes                              | Text generation (summarization, translation)     |\n",
        "    # | `T5Model`                      | Yes             | No                               | Feature extraction, embeddings                   |\n",
        "    # | `T5EncoderModel`               | Encoder only    | No                               | Classification, regression                       |\n",
        "\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "x-1pBmuOC3Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "def load_imdb_data():\n",
        "    dataset = load_dataset(\"imdb\")\n",
        "    train = dataset['train'].to_pandas()\n",
        "    test = dataset['test'].to_pandas()\n",
        "    unsupervised = dataset['unsupervised'].to_pandas()\n",
        "    return {'train': train, 'test': test}#,'unsupervised':unsupervised}\n",
        "\n",
        "def label_map(label):\n",
        "    if label == 0:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'positive'\n",
        "\n",
        "\n",
        "\n",
        "dataset = load_imdb_data()\n",
        "train = dataset['train']\n",
        "test = dataset['test']\n",
        "tqdm.pandas()\n",
        "train['label'] = train['label'].progress_apply(label_map)\n",
        "test['label'] = test['label'].progress_apply(label_map)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_kmWjTkC7lS",
        "outputId": "2b8f30d1-cbc1-483b-c04c-c74ad4280a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "100%|██████████| 25000/25000 [00:00<00:00, 1060206.46it/s]\n",
            "100%|██████████| 25000/25000 [00:00<00:00, 1089022.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(\n",
        "      text,\n",
        "      remove_stopwords=False,\n",
        "      remove_punctuation=False,\n",
        "      remove_numbers=False,\n",
        "    ):\n",
        "    text = text.lower()\n",
        "    if remove_punctuation:\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "\n",
        "    if remove_numbers:\n",
        "        text = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    if remove_stopwords:\n",
        "        text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "    return text"
      ],
      "metadata": {
        "id": "ECrpKMhREF1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_frequent_words(text_series, n=10):\n",
        "  words = []\n",
        "  for text in tqdm(text_series):\n",
        "      words.extend([word for word in re.findall(r'\\w+', text.lower()) if word not in stop_words])\n",
        "  most_common_words = Counter(words).most_common(n)\n",
        "  return most_common_words\n",
        "\n",
        "\n",
        "def get_top_ngrams(text_series, n=8, top_k=50):\n",
        "    ngrams = []\n",
        "    for text in tqdm(text_series):\n",
        "        tokens = [word for word in re.findall(r'\\w+', text.lower())]\n",
        "        ngrams.extend([\" \".join(tokens[i:i+n]) for i in range(len(tokens)-n+1)])\n",
        "    most_common_ngrams = Counter(ngrams).most_common(top_k)\n",
        "    return most_common_ngrams\n",
        "\n",
        "\n",
        "most_common_words = get_most_frequent_words(train['text'])\n",
        "most_common_ngrams = get_top_ngrams(train['text'])\n",
        "print(most_common_ngrams)\n",
        "print(most_common_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIhZ5X6JEoKB",
        "outputId": "7059fe70-e4d7-4b49-dde1-8a7e8f607cfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25000/25000 [00:02<00:00, 10737.70it/s]\n",
            "100%|██████████| 25000/25000 [00:04<00:00, 5853.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('one of the worst movies i have ever', 45), ('of the worst movies i have ever seen', 44), ('one of the worst movies i ve ever', 29), ('of the worst movies i ve ever seen', 28), ('this is one of the worst movies i', 23), ('one of the worst films i have ever', 22), ('is one of the worst movies i have', 21), ('of the worst films i have ever seen', 20), ('my vote is eight br br title brazil', 17), ('is the worst movie i have ever seen', 15), ('don t say i didn t warn you', 15), ('br br don t get me wrong i', 15), ('one of the worst films i ve ever', 15), ('br br if you re looking for a', 14), ('this has to be one of the worst', 13), ('my vote is seven br br title brazil', 13), ('don t waste your time or money on', 12), ('br br if you want to see a', 12), ('of the worst films i ve ever seen', 12), ('i went out after and bought a case', 12), ('went out after and bought a case of', 12), ('out after and bought a case of cognac', 12), ('t waste your time or money on this', 11), ('don t even get me started on the', 11), ('to be one of the worst movies i', 11), ('according to the dvd sleeve s synopsis br', 11), ('to the dvd sleeve s synopsis br br', 11), ('the worst movie i have ever seen in', 11), ('movie i have ever seen in my life', 11), ('i would have liked to have seen more', 11), ('one of the best movies i ve seen', 11), ('after and bought a case of cognac i', 11), ('and bought a case of cognac i went', 11), ('bought a case of cognac i went out', 11), ('a case of cognac i went out after', 11), ('case of cognac i went out after and', 11), ('of cognac i went out after and bought', 11), ('cognac i went out after and bought a', 11), ('the only good thing about this movie is', 10), ('the worst movie i have ever seen i', 10), ('br br i could go on and on', 10), ('this is the worst movie i have ever', 10), ('one of the worst movies i ve seen', 10), ('worst movie i have ever seen in my', 10), ('br br all in all this is a', 10), ('is one of the worst movies i ve', 10), ('the night evelyn came out of the grave', 10), ('one of my favorite movies of all time', 10), ('is one of the worst movies ever made', 9), ('br br if you are looking for a', 9)]\n",
            "[('br', 101871), ('movie', 44047), ('film', 40159), ('one', 26795), ('like', 20281), ('good', 15147), ('time', 12727), ('even', 12655), ('would', 12436), ('story', 11988)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data.iloc[idx][\"text\"]\n",
        "        label = self.data.iloc[idx][\"label\"]\n",
        "\n",
        "        text = \"Answer this question just with positive or negative words.What is the sentiment of this review? \" + text\n",
        "\n",
        "\n",
        "        # Tokenize the text input\n",
        "        text_encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            padding='longest',  # Dynamic padding\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        # Tokenize the label as a text input\n",
        "        label_encoding = self.tokenizer.encode_plus(\n",
        "            label,\n",
        "            return_tensors='pt',\n",
        "            padding='longest',  # Dynamic padding\n",
        "            truncation=True,\n",
        "            max_length=8,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": text_encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": text_encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"label_input_ids\": label_encoding[\"input_ids\"].squeeze(0),\n",
        "            \"label_attention_mask\": label_encoding[\"attention_mask\"].squeeze(0)\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    max_input_length = max(item['input_ids'].size(0) for item in batch)\n",
        "    max_label_length = max(item['label_input_ids'].size(0) for item in batch)\n",
        "    def pad_sequence(sequence, max_len, pad_value=0):\n",
        "        return torch.nn.functional.pad(sequence, (0, max_len - sequence.size(0)), value=pad_value)\n",
        "    input_ids = torch.stack([pad_sequence(item['input_ids'], max_input_length) for item in batch])\n",
        "    attention_mask = torch.stack([pad_sequence(item['attention_mask'], max_input_length) for item in batch])\n",
        "    label_input_ids = torch.stack([pad_sequence(item['label_input_ids'], max_label_length) for item in batch])\n",
        "    label_attention_mask = torch.stack([pad_sequence(item['label_attention_mask'], max_label_length) for item in batch])\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask,\n",
        "        'label_input_ids': label_input_ids,\n",
        "        'label_attention_mask': label_attention_mask,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "bV9Yq2Nhcuxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SoftPrompt(nn.Module):\n",
        "    def __init__(self, n_tokens, embedding_layer, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = embedding_layer\n",
        "        self.prompt_embedding = nn.Parameter(torch.randn(n_tokens, hidden_size))\n",
        "        self.n_tokens = n_tokens\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        with torch.no_grad():\n",
        "            input_embeddings = self.embedding(input_ids)\n",
        "        batch_size = input_embeddings.size(0)\n",
        "        prompt_embeddings = self.prompt_embedding.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        combined_embeddings = torch.cat([prompt_embeddings, input_embeddings], dim=1)\n",
        "        return combined_embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "r9C5lESvR1Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model , tokenizer = load_model()\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "soft = SoftPrompt(\n",
        "    n_tokens=10,\n",
        "    embedding_layer=model.encoder.embed_tokens,\n",
        "    hidden_size=model.config.hidden_size\n",
        ")\n",
        "model.encoder.embed_tokens = soft\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.encoder.embed_tokens.parameters()),\n",
        "    lr=0.001,\n",
        "    fused=True\n",
        ")\n",
        "\n",
        "trainset = IMDBDataset(train, tokenizer, max_length=384)\n",
        "testset = IMDBDataset(test, tokenizer, max_length=384)\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True,collate_fn=collate_fn)\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=False,collate_fn=collate_fn)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "y7kbfs4tfvqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c351ad1-1ae8-41eb-8684-bf3900ff72e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def adjust_attention_mask(attention_mask, n_tokens=10):\n",
        "    batch_size = attention_mask.size(0)\n",
        "    soft_prompt_mask = torch.ones((batch_size, n_tokens), dtype=attention_mask.dtype, device=attention_mask.device)\n",
        "    adjusted_attention_mask = torch.cat([soft_prompt_mask, attention_mask], dim=1)\n",
        "    return adjusted_attention_mask\n",
        "\n",
        "\n",
        "def train_step(model, data_loader, optimizer, device, mixed_precision=False):\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler() if mixed_precision else None\n",
        "\n",
        "    for batch in tqdm(data_loader, desc=\"Training\", leave=False):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        attention_mask = adjust_attention_mask(attention_mask)\n",
        "        label_input_ids = batch['label_input_ids'].to(device)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mixed_precision:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=label_input_ids)\n",
        "                loss = outputs.loss\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=label_input_ids)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"Training Step Time: {epoch_time:.2f} seconds\")\n",
        "    return running_loss / len(data_loader)\n",
        "\n",
        "def eval_step(model, data_loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\", leave=False):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            attention_mask = adjust_attention_mask(attention_mask)\n",
        "\n",
        "            label_input_ids = batch['label_input_ids'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask,labels=label_input_ids)\n",
        "            loss = outputs.loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=-1).view(-1).cpu().numpy()\n",
        "            labels = label_input_ids.view(-1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels)\n",
        "\n",
        "    eval_time = time.time() - start_time\n",
        "    print(f\"Evaluation Step Time: {eval_time:.2f} seconds\")\n",
        "    return running_loss / len(data_loader), all_preds, all_labels\n",
        "\n",
        "def compute_metrics(all_preds, all_labels):\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
        "    recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\" - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\" - Precision: {precision:.4f}\")\n",
        "    print(f\" - Recall: {recall:.4f}\")\n",
        "    print(f\" - F1 Score: {f1:.4f}\")\n",
        "\n",
        "def train_and_evaluate(model, train_loader, test_loader, optimizer, device, epochs=3, mixed_precision=False, compile_model=False):\n",
        "    import torch._dynamo\n",
        "    torch._dynamo.config.suppress_errors = True\n",
        "    if compile_model:\n",
        "        print(\"Compiling model...\")\n",
        "        model = torch.compile(model)\n",
        "    else:\n",
        "        print(\"Not compiling model.\")\n",
        "\n",
        "    if mixed_precision:\n",
        "      print(\"Using mixed precision training\")\n",
        "    else:\n",
        "      print(\"Not using mixed precision training\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss = train_step(model, train_loader, optimizer, device, mixed_precision=mixed_precision)\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        eval_loss, all_preds, all_labels = eval_step(model, test_loader, device)\n",
        "        print(f\"Eval Loss: {eval_loss:.4f}\")\n",
        "\n",
        "        compute_metrics(all_preds, all_labels)\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        print(f\"Epoch Time: {epoch_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "5ZLD2awjgTYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Mixed Precision and compiling the model\n",
        "train_and_evaluate(\n",
        "    model=model,\n",
        "    train_loader=trainloader,\n",
        "    test_loader=testloader,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    epochs=5,\n",
        "    mixed_precision=True,      # Set to True to enable mixed precision\n",
        "    compile_model=False         # Set to True to enable torch.compile\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUGH8rfIooHc",
        "outputId": "99d354df-a546-4e8c-e44e-4637c00fa2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-946f5998bfa2>:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if mixed_precision else None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not compiling model.\n",
            "Using mixed precision training\n",
            "\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/196 [00:00<?, ?it/s]<ipython-input-11-946f5998bfa2>:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step Time: 176.24 seconds\n",
            "Train Loss: 10.1323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "<ipython-input-11-946f5998bfa2>:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if mixed_precision else None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Step Time: 154.90 seconds\n",
            "Eval Loss: 7.1349\n",
            "Evaluation Metrics:\n",
            " - Accuracy: 0.0004\n",
            " - Precision: 0.1957\n",
            " - Recall: 0.0004\n",
            " - F1 Score: 0.0007\n",
            "Epoch Time: 331.30 seconds\n",
            "\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/196 [00:00<?, ?it/s]<ipython-input-11-946f5998bfa2>:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step Time: 175.22 seconds\n",
            "Train Loss: 6.8440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "<ipython-input-11-946f5998bfa2>:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if mixed_precision else None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Step Time: 155.53 seconds\n",
            "Eval Loss: 1.7386\n",
            "Evaluation Metrics:\n",
            " - Accuracy: 0.4177\n",
            " - Precision: 0.8512\n",
            " - Recall: 0.4177\n",
            " - F1 Score: 0.4953\n",
            "Epoch Time: 330.90 seconds\n",
            "\n",
            "Epoch 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/196 [00:00<?, ?it/s]<ipython-input-11-946f5998bfa2>:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step Time: 174.88 seconds\n",
            "Train Loss: 2.3634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "<ipython-input-11-946f5998bfa2>:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if mixed_precision else None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Step Time: 155.29 seconds\n",
            "Eval Loss: 0.8051\n",
            "Evaluation Metrics:\n",
            " - Accuracy: 0.5901\n",
            " - Precision: 0.8839\n",
            " - Recall: 0.5901\n",
            " - F1 Score: 0.6166\n",
            "Epoch Time: 330.32 seconds\n",
            "\n",
            "Epoch 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/196 [00:00<?, ?it/s]<ipython-input-11-946f5998bfa2>:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step Time: 175.73 seconds\n",
            "Train Loss: 1.0824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "<ipython-input-11-946f5998bfa2>:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if mixed_precision else None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Step Time: 156.00 seconds\n",
            "Eval Loss: 0.4360\n",
            "Evaluation Metrics:\n",
            " - Accuracy: 0.7632\n",
            " - Precision: 0.8751\n",
            " - Recall: 0.7632\n",
            " - F1 Score: 0.7146\n",
            "Epoch Time: 331.88 seconds\n",
            "\n",
            "Epoch 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/196 [00:00<?, ?it/s]<ipython-input-11-946f5998bfa2>:32: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Training:  64%|██████▍   | 126/196 [01:52<01:00,  1.16it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from transformers.models.t5.modeling_t5 import T5Attention\n",
        "\n",
        "class T5FlashAttention(T5Attention):\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        mask=None,\n",
        "        key_value_states=None,\n",
        "        position_bias=None,\n",
        "        past_key_value=None,\n",
        "        layer_head_mask=None,\n",
        "        query_length=None,\n",
        "        use_cache=False,\n",
        "        output_attentions=False,\n",
        "        cache_position=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n",
        "        \"\"\"\n",
        "        # Input is (batch_size, seq_length, dim)\n",
        "        # Mask is (batch_size, 1, 1, key_length) (non-causal encoder) or (batch_size, 1, seq_length, key_length) (causal decoder)\n",
        "        batch_size, seq_length = hidden_states.shape[:2]\n",
        "\n",
        "        # if key_value_states are provided this layer is used as a cross-attention layer for the decoder\n",
        "        is_cross_attention = key_value_states is not None\n",
        "\n",
        "        query_states = self.q(hidden_states)\n",
        "        query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
        "\n",
        "        if past_key_value is not None:\n",
        "            is_updated = past_key_value.is_updated.get(self.layer_idx)\n",
        "            if is_cross_attention:\n",
        "                # after the first generated id, we can subsequently re-use all key/value_states from cache\n",
        "                curr_past_key_value = past_key_value.cross_attention_cache\n",
        "            else:\n",
        "                curr_past_key_value = past_key_value.self_attention_cache\n",
        "\n",
        "        current_states = key_value_states if is_cross_attention else hidden_states\n",
        "        if is_cross_attention and past_key_value is not None and is_updated:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_states = curr_past_key_value.key_cache[self.layer_idx]\n",
        "            value_states = curr_past_key_value.value_cache[self.layer_idx]\n",
        "        else:\n",
        "            key_states = self.k(current_states)\n",
        "            value_states = self.v(current_states)\n",
        "            key_states = key_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
        "            value_states = value_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n",
        "\n",
        "            if past_key_value is not None:\n",
        "                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n",
        "                cache_position = cache_position if not is_cross_attention else None\n",
        "                key_states, value_states = curr_past_key_value.update(\n",
        "                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n",
        "                )\n",
        "                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n",
        "                if is_cross_attention:\n",
        "                    past_key_value.is_updated[self.layer_idx] = True\n",
        "\n",
        "        # Compute scaled dot product attention using F.scaled_dot_product_attention\n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            query_states, key_states, value_states, attn_mask=mask, dropout_p=self.dropout if self.training else 0.0\n",
        "        )\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n",
        "\n",
        "        attn_output = self.o(attn_output)\n",
        "\n",
        "        outputs = (attn_output, past_key_value, position_bias)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs = outputs + (None,)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "5F-U-evsUnYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in model.encoder.block:\n",
        "    layer.layer[0].SelfAttention = T5FlashAttention(model.config)\n",
        "for layer in model.decoder.block:\n",
        "    layer.layer[0].SelfAttention = T5FlashAttention(model.config)\n",
        "    layer.layer[1].EncDecAttention = T5FlashAttention(model.config)"
      ],
      "metadata": {
        "id": "_W6JJkmqUojq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Replace Attention with FlashAttention for optimize the calculations\")\n",
        "train_and_evaluate(\n",
        "    model=model,\n",
        "    train_loader=trainloader,\n",
        "    test_loader=testloader,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    epochs=5,\n",
        "    mixed_precision=True,      # Set to True to enable mixed precision\n",
        "    compile_model=False         # Set to True to enable torch.compile\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "TdZoeBiTx5bQ",
        "outputId": "921eae39-8bbb-4718-f65f-efc544b0b4d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text     label\n",
              "0      I love sci-fi and am willing to put up with a ...  negative\n",
              "1      Worth the entertainment value of a rental, esp...  negative\n",
              "2      its a totally average film with a few semi-alr...  negative\n",
              "3      STAR RATING: ***** Saturday Night **** Friday ...  negative\n",
              "4      First off let me say, If you haven't enjoyed a...  negative\n",
              "...                                                  ...       ...\n",
              "24995  Just got around to seeing Monster Man yesterda...  positive\n",
              "24996  I got this as part of a competition prize. I w...  positive\n",
              "24997  I got Monster Man in a box set of three films ...  positive\n",
              "24998  Five minutes in, i started to feel how naff th...  positive\n",
              "24999  I caught this movie on the Sci-Fi channel rece...  positive\n",
              "\n",
              "[25000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-137af44f-ee16-46b2-80cc-1bf89ab4c0c4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I love sci-fi and am willing to put up with a ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Worth the entertainment value of a rental, esp...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>its a totally average film with a few semi-alr...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>STAR RATING: ***** Saturday Night **** Friday ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>First off let me say, If you haven't enjoyed a...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24995</th>\n",
              "      <td>Just got around to seeing Monster Man yesterda...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24996</th>\n",
              "      <td>I got this as part of a competition prize. I w...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24997</th>\n",
              "      <td>I got Monster Man in a box set of three films ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24998</th>\n",
              "      <td>Five minutes in, i started to feel how naff th...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24999</th>\n",
              "      <td>I caught this movie on the Sci-Fi channel rece...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-137af44f-ee16-46b2-80cc-1bf89ab4c0c4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-137af44f-ee16-46b2-80cc-1bf89ab4c0c4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-137af44f-ee16-46b2-80cc-1bf89ab4c0c4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-63e701af-22a7-4414-b1b4-48b530182b9f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-63e701af-22a7-4414-b1b4-48b530182b9f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-63e701af-22a7-4414-b1b4-48b530182b9f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_0ef2b563-e54d-47d6-a141-6f04d5e53532\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_0ef2b563-e54d-47d6-a141-6f04d5e53532 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test",
              "summary": "{\n  \"name\": \"test\",\n  \"rows\": 25000,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24801,\n        \"samples\": [\n          \"Most movies I can sit through easily, even if I do not particularly like the movie. I am the type of person who recognizes great films even if I do not like the genre. This is the first movie I could not stand to watch. Cat in the Hat is the worst movie I have ever seen--and I've seen a lot of movies. The acting is okay (Myers is good as the cat, it's just that he is REALLY annoying). The silly songs the cat sings were boring and monotonous, even for the children in the audience. The plot drags on and on, and viewers must suffer through poor dialogue. The \\\"witty\\\" parental remarks are disgusting, not funny (I remember some awful comment about a garden hoe being compared to, well, a type of person people call a \\\"ho\\\"). Even though the movie is really short, it seemed to last FOREVER. Do not waste your time. I know small kids who hated this movie. If children can't stand it, I do not know how any adults can. I would like to fume more about this film but I do not even feel like wasting anymore time writing this review about it. I HATED IT! So, in summary, do not spend 90 minutes of your life watching this! See a GOOD movie!<br /><br />1/10 stars--the lowest review I have ever given a movie.\",\n          \"it's the best film that takes the first place at the sickest and an useful films ever made in this poor country. really u people even don't know what the word cinema means and u cast votes for movies, i'm really curious to know how many movies of P.P.P. or L.Bunuel have u seen. The score of this \\\"faield experiment\\\" it reflects a lot of u're way of understanding things and to recognize a good/quality movie when u see it. We the Romanian people have only ONE movie and until this day the status hasn't change & that movie is \\\"Padurea Spanzuratilor\\\". But I don't lose my time thinking how many of you have seen it. That is a movie that respects all the required quality's's of an MOVIE. From the script to the frames and even to the quality/clarity of the picture even are past over him 41 years. I recommend to the voters to search for better movies and then vote. KuDos will See u later .\",\n          \"\\\"Rush in Rio\\\" is, no doubt, one of the most exciting DVDs I have purchased. Although I am a biased Rush fan of almost 20 years, I found this performance to be flawless. The music is heavy and sharp (which sounds great on any surround sound system), the band is energetic, the crowd has a constant smile... it's like they were able to capture every concert I've been to. For any Rush fan, this DVD is a must; if anything, just to see the \\\"Boys in Brazil\\\" documentary (which reveals the travels of this rather isolated, personal band). For any non-Rush fan, this DVD is an enjoyable concert. Rush fans know the talent of these three Canadians. We have rather firmly stood by them for years. I've shown this DVD (or portions of it, anyway) to those who have never heard of Rush, or those who think Rush is less than good because they do not appeal to the pessimistic masses of rock (i.e., sex, drugs, and a drunken frenzy). The bottom line is this DVD is worth every penny and more than worth the time to view it.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"positive\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gmt339v1_uNO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}